---
title: "Robust and resistance regression"
output: html_notebook
---


# Theory

Based on https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Robust-Regression.pdf

## M-Estimation

The most common general method of robust regression is M-estimation, introduced by Huber (1964). This class of estimators can be regarded as a generalization of maximum-likelihood estimation, hence the "M".

We consider only the linear model 

$$
y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \epsilon_i
$$
for the $i$-th observation of $n$ independent observations. We assume that the model itself is not at issue, $E(y |\boldsymbol{x}) =  \boldsymbol{x}_i'\boldsymbol{\beta}$, but the distribution of the errors may be heavy-tailed, producing occasional outliers. 

Residuals are given by

$$
e_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i'\boldsymbol{b}
$$
In M -estimation, the estimates $\boldsymbol{b}$ are determined by minimizing a particular objective function over all $\boldsymbol{b}$.

$$
\sum_{i=1}^n\rho(e_i) = \sum_{i=1}^n\rho(y_i - \boldsymbol{x}_i'\boldsymbol{b}),
$$

where function $\rho$ should have the following properties:

+ always-non negative,
+ $\rho(0) = 0$,
+ symmetric -- $\rho(-e) = \rho(e)$,
+ monotone in $|e_i|$, $\rho(e_i) \geq \rho(e_i')$ for $|e_i| \geq |e_i'|$

For example, the least-squares $\rho(e_i) = e_i^2$ satisfies these requirements, as do many other
functions.

## Computing of M-estimates

The minimum value of Equation 1 can be found by differentiating with respect to the argument b,
and setting the resulting partial derivatives to 

$$
\boldsymbol{0} = \frac{\partial}{\partial \boldsymbol{b}} \sum_{i=1}^{n}\rho(y_i - \boldsymbol{x}_i^\prime\boldsymbol{b}) = \sum_{i=1}^n\psi(y_i - \boldsymbol{x}_i'\boldsymbol{b})\boldsymbol{x}_i'
$$
where the influence curve $\psi$ is is defined to be the derivative of $\rho$ with respect to its argument.

To facilitate computing, we would like to make this last equation similar to the estimating equations for a familiar problem like weighted least squares. To this end, define the weight function $w_i=w(e_i)=\psi(e_i)/e_i$

$$
\sum_{i=1}^nw(y_i - \boldsymbol{x}_i'\boldsymbol{b})\boldsymbol{x}_i' = \boldsymbol{0}
$$
An iterative solution (called iteratively reweighted least-squares, IRLS) is therefore required:

1. Select initial estimates $\boldsymbol{b}^{0}$, such as the least-squares estimates.
2. At each iteration $t$,  calculate residuals $e_i^{(t-1)}$ and associated weights $w_i^{(t-1)}=w\left[e_i^{t=1} \right]$.
3. Solve for new weighted-least-squares estimates

$$
\boldsymbol{b}^{(t)} = \left[\boldsymbol{X}'\boldsymbol{W}^{(t-1)}\boldsymbol{X}\right]^{-1}\boldsymbol{X}'\boldsymbol{W}^{(t-1)}\boldsymbol{y},
$$

where $\boldsymbol{X}$ is the model matrix, with $\boldsymbol{x}_i'$ as its $i$th row, and $\boldsymbol{W}^{(t-1)} = diag\left\{ w_{i}^{t-1}\right\}$ in the current weight matrix.

Steps 2 and 3 are repeated until the estimated coefficients converge.

![objective functions](m-functions.png)

# Bounded-Influence Regression

Under certain circumstances, M-estimators can be vulnerable to high-leverage observations. Very- high-breakdown bounded-influence estimators for regression have been proposed.

One bounded-influence estimator is least-trimmed squares (LTS) regression. Order the squared residuals from smallest to largest

$$
(e^2)_{(1)},(e^2)_{(2)},...,(e^2)_{(n)}
$$

The LTS estimator chooses the regression coefficients $\boldsymbol{b}$ to minimize the sum of the smallest $m$ of the squared residuals.

$$
LTS(\boldsymbol{b}) = \sum_{i=1}^m(e^2)_{(i)},
$$
where typicaly $m = \lfloor n/2 \rfloor + \lfloor (k+2)/2 \rfloor$. While the LTS criterion is easily described, the mechanics of fitting the LTS estimator are complicated. Moreover, bounded-influence estimators can produce unreasonable results in certain circumstances and there is no simple formula for coefficient standard errors.

One application of bounded-influence estimators is to provide starting values for M -estimation. This procedure, along with using the bounded-influence estimate of the error variance, produces the so-called MM-estimator. The MM -estimator retains the high breakdown point of the bounded- influence estimator and shares the relatively high efficiency under normality of the traditional M -estimator. MM -estimators are especially attractive when paired with redescending ùúì-functions such as the bisquare, which can be sensitive to starting values.

# Example

Functions:

+ MASS:rlm -- robust linear regession (e.g. the Huber M-estimator)

```{r}
rema <- read.csv2('../data-raw/project_data_train.csv', 
                 header = T, 
                 stringsAsFactors = F, 
                 dec = '.')
rema
```

## Linear regression model

```{r}
model1 <- lm(trans_dwelling_price ~ trans_dwelling_floor_area, data = rema)
summary(model1)
```

## Robust LM: M-estimation approach

```{r}
model2 <- MASS::rlm(trans_dwelling_price ~ trans_dwelling_floor_area, data = rema)
summary(model2)
```

```{r}
hist(model2$w)
```

```{r}
plot(rema$trans_dwelling_floor_area,
     rema$trans_dwelling_price,
     xlab = 'Floor area',
     ylab = 'Price')
abline(coef(model1), col = 'red')
abline(coef(model2), col = 'blue')
```

## Robust LM: MM-estimation approach

```{r}
model3 <- MASS::rlm(trans_dwelling_price ~ trans_dwelling_floor_area, 
                    data = rema, method = 'MM')
summary(model3)
```


```{r}
model3 <- robust::lmRob(trans_dwelling_price ~ trans_dwelling_floor_area,
                        data = rema)
summary(model3)
```

`The ‚Äòtest for bias‚Äô is of the M-estimator against the initial S-estimator; if the M-estimator appears biased the initial S-estimator is returned.

``{r}
model4 <- robustbase::lmrob(trans_dwelling_price ~ trans_dwelling_floor_area,
                            data = rema)
summary(model4)
```


## ltsReg
```{r}
model5 <- robustbase::ltsReg(trans_dwelling_price ~ trans_dwelling_floor_area,
                            data = rema)
summary(model5)
```

`